{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":8419781,"sourceType":"datasetVersion","datasetId":5012409},{"sourceId":8754088,"sourceType":"datasetVersion","datasetId":5258823}],"dockerImageVersionId":30699,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2024-07-06T06:04:31.106762Z","iopub.execute_input":"2024-07-06T06:04:31.107029Z","iopub.status.idle":"2024-07-06T06:04:32.008695Z","shell.execute_reply.started":"2024-07-06T06:04:31.107004Z","shell.execute_reply":"2024-07-06T06:04:32.007669Z"},"trusted":true},"execution_count":1,"outputs":[{"name":"stdout","text":"/kaggle/input/chat-data/chat_data.json\n/kaggle/input/last-data/chat_data.json\n","output_type":"stream"}]},{"cell_type":"code","source":"import pandas as pd\nimport json\n# Load your data\nwith open('/kaggle/input/last-data/chat_data.json', 'r') as file:\n    data = json.load(file)['data']\n\n# Prepare the data\ntrain_data = []\nfor item in data:\n    for pattern in item['patterns']:\n        for response in item['responses']:\n            train_data.append({\"prompt\": pattern, \"response\": response})\n\n            \ndf = pd.DataFrame(train_data)\ndf = df.drop_duplicates()\ndf.head()\n","metadata":{"execution":{"iopub.status.busy":"2024-07-06T06:06:08.313312Z","iopub.execute_input":"2024-07-06T06:06:08.314270Z","iopub.status.idle":"2024-07-06T06:06:08.346692Z","shell.execute_reply.started":"2024-07-06T06:06:08.314240Z","shell.execute_reply":"2024-07-06T06:06:08.345819Z"},"trusted":true},"execution_count":2,"outputs":[{"execution_count":2,"output_type":"execute_result","data":{"text/plain":"  prompt  response\n0     Hi     Hello\n1     Hi        Hi\n2     Hi  Hi there\n3     Hi       Hey\n4    Hey     Hello","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>prompt</th>\n      <th>response</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>Hi</td>\n      <td>Hello</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>Hi</td>\n      <td>Hi</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>Hi</td>\n      <td>Hi there</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>Hi</td>\n      <td>Hey</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>Hey</td>\n      <td>Hello</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}]},{"cell_type":"code","source":"# Split the data into train and test sets, with 90% in the train set\ntrain_df = df.sample(frac=0.9, random_state=42)\ntest_df = df.drop(train_df.index)\n\n# Save the dataframes to .jsonl files\ntrain_df.to_json('train.json', orient='records', lines=True)\ntest_df.to_json('test.json', orient='records', lines=True)","metadata":{"execution":{"iopub.status.busy":"2024-07-06T06:06:09.485997Z","iopub.execute_input":"2024-07-06T06:06:09.486602Z","iopub.status.idle":"2024-07-06T06:06:09.499808Z","shell.execute_reply.started":"2024-07-06T06:06:09.486570Z","shell.execute_reply":"2024-07-06T06:06:09.498770Z"},"trusted":true},"execution_count":3,"outputs":[]},{"cell_type":"code","source":"!pip install -q accelerate==0.21.0 peft==0.4.0 bitsandbytes==0.40.2 transformers==4.33.1 trl==0.4.7\nimport os\nimport torch\nfrom datasets import load_dataset\nfrom transformers import (\n    AutoModelForCausalLM,\n    AutoTokenizer,\n    BitsAndBytesConfig,\n    HfArgumentParser,\n    TrainingArguments,\n    pipeline,\n    logging,\n)\nfrom peft import LoraConfig, PeftModel\nfrom trl import SFTTrainer","metadata":{"execution":{"iopub.status.busy":"2024-07-06T06:06:09.696664Z","iopub.execute_input":"2024-07-06T06:06:09.697419Z","iopub.status.idle":"2024-07-06T06:06:57.596713Z","shell.execute_reply.started":"2024-07-06T06:06:09.697389Z","shell.execute_reply":"2024-07-06T06:06:57.595864Z"},"trusted":true},"execution_count":4,"outputs":[{"name":"stderr","text":"2024-07-06 06:06:44.996184: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n2024-07-06 06:06:44.996328: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n2024-07-06 06:06:45.125316: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n","output_type":"stream"}]},{"cell_type":"code","source":"model_name = \"NousResearch/llama-2-7b-chat-hf\" \ndataset_name = \"/kaggle/working/train.json\"\nnew_model = \"SmartTour_llama-2-7b\"\nlora_r = 64\nlora_alpha = 16\nlora_dropout = 0.1\nuse_4bit = True\nbnb_4bit_compute_dtype = \"float16\"\nbnb_4bit_quant_type = \"nf4\"\nuse_nested_quant = False\noutput_dir = \"./results\"\nnum_train_epochs = 2\nfp16 = False\nbf16 = False\nper_device_train_batch_size = 2\nper_device_eval_batch_size = 2\ngradient_accumulation_steps = 2\ngradient_checkpointing = True\nmax_grad_norm = 0.3\nlearning_rate = 2e-4\nweight_decay = 0.001\noptim = \"paged_adamw_32bit\"\nlr_scheduler_type = \"constant\"\nmax_steps = -1\nwarmup_ratio = 0.03\ngroup_by_length = True\nsave_steps = 25\nlogging_steps = 5\nmax_seq_length = None\npacking = False\ndevice_map = {\"\": 0}\n\n\n","metadata":{"execution":{"iopub.status.busy":"2024-07-06T06:27:41.947136Z","iopub.execute_input":"2024-07-06T06:27:41.948005Z","iopub.status.idle":"2024-07-06T06:27:41.956896Z","shell.execute_reply.started":"2024-07-06T06:27:41.947964Z","shell.execute_reply":"2024-07-06T06:27:41.955884Z"},"trusted":true},"execution_count":10,"outputs":[]},{"cell_type":"code","source":"# Load datasets\ntrain_dataset = load_dataset('json', data_files='/kaggle/working/train.json', split=\"train\")\nvalid_dataset = load_dataset('json', data_files='//kaggle/working/test.json', split=\"train\")\n\n# Preprocess datasets|\nsystem_message = \"Your system message goes here\"\ntrain_dataset_mapped = train_dataset.map(lambda examples: {'text': [f'[INST] <<SYS>>\\n{system_message.strip()}\\n<</SYS>>\\n\\n' + prompt + ' [/INST] ' + response for prompt, response in zip(examples['prompt'], examples['response'])]}, batched=True)\nvalid_dataset_mapped = valid_dataset.map(lambda examples: {'text': [f'[INST] <<SYS>>\\n{system_message.strip()}\\n<</SYS>>\\n\\n' + prompt + ' [/INST] ' + response for prompt, response in zip(examples['prompt'], examples['response'])]}, batched=True)\n","metadata":{"execution":{"iopub.status.busy":"2024-07-06T06:27:42.404507Z","iopub.execute_input":"2024-07-06T06:27:42.405357Z","iopub.status.idle":"2024-07-06T06:27:43.019303Z","shell.execute_reply.started":"2024-07-06T06:27:42.405315Z","shell.execute_reply":"2024-07-06T06:27:43.018167Z"},"trusted":true},"execution_count":11,"outputs":[]},{"cell_type":"code","source":"compute_dtype = getattr(torch, bnb_4bit_compute_dtype)\nbnb_config = BitsAndBytesConfig(\n    load_in_4bit=use_4bit,\n    bnb_4bit_quant_type=bnb_4bit_quant_type,\n    bnb_4bit_compute_dtype=compute_dtype,\n    bnb_4bit_use_double_quant=use_nested_quant,\n)\nmodel = AutoModelForCausalLM.from_pretrained(\n    model_name,\n    quantization_config=bnb_config,\n    device_map=device_map\n)\nmodel.config.use_cache = False\nmodel.config.pretraining_tp = 1\ntokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)\ntokenizer.pad_token = tokenizer.eos_token\ntokenizer.padding_side = \"right\"\npeft_config = LoraConfig(\n    lora_alpha=lora_alpha,\n    lora_dropout=lora_dropout,\n    r=lora_r,\n    bias=\"none\",\n    task_type=\"CAUSAL_LM\",\n)\n\n","metadata":{"execution":{"iopub.status.busy":"2024-07-06T06:27:43.021001Z","iopub.execute_input":"2024-07-06T06:27:43.021386Z","iopub.status.idle":"2024-07-06T06:27:51.856438Z","shell.execute_reply.started":"2024-07-06T06:27:43.021348Z","shell.execute_reply":"2024-07-06T06:27:51.855241Z"},"trusted":true},"execution_count":12,"outputs":[{"output_type":"display_data","data":{"text/plain":"Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"05a99ab260c14fecb2a837ec27654795"}},"metadata":{}}]},{"cell_type":"code","source":"# Set training parameters\ntraining_arguments = TrainingArguments(\n    output_dir=output_dir,\n    num_train_epochs=num_train_epochs,\n    per_device_train_batch_size=per_device_train_batch_size,\n    gradient_accumulation_steps=gradient_accumulation_steps,\n    optim=optim,\n    save_steps=save_steps,\n    logging_steps=logging_steps,\n    learning_rate=learning_rate,\n    weight_decay=weight_decay,\n    fp16=fp16,\n    bf16=bf16,\n    max_grad_norm=max_grad_norm,\n    max_steps=max_steps,\n    warmup_ratio=warmup_ratio,\n    group_by_length=group_by_length,\n    lr_scheduler_type=lr_scheduler_type,\n    report_to=\"all\",\n    evaluation_strategy=\"steps\",\n    eval_steps=5  # Evaluate every 20 steps\n)\n","metadata":{"execution":{"iopub.status.busy":"2024-07-06T06:27:51.858411Z","iopub.execute_input":"2024-07-06T06:27:51.858814Z","iopub.status.idle":"2024-07-06T06:27:51.868997Z","shell.execute_reply.started":"2024-07-06T06:27:51.858767Z","shell.execute_reply":"2024-07-06T06:27:51.868200Z"},"trusted":true},"execution_count":13,"outputs":[]},{"cell_type":"code","source":"# Set supervised fine-tuning parameters\ntrainer = SFTTrainer(\n    model=model,\n    train_dataset=train_dataset_mapped,\n    eval_dataset=valid_dataset_mapped,  # Pass validation dataset here\n    peft_config=peft_config,\n    dataset_text_field=\"text\",\n    max_seq_length=max_seq_length,\n    tokenizer=tokenizer,\n    args=training_arguments,\n    packing=packing,\n)\ntrainer.train()\ntrainer.model.save_pretrained(new_model)\n","metadata":{"execution":{"iopub.status.busy":"2024-07-06T06:27:51.870082Z","iopub.execute_input":"2024-07-06T06:27:51.870388Z","iopub.status.idle":"2024-07-06T06:37:28.800510Z","shell.execute_reply.started":"2024-07-06T06:27:51.870363Z","shell.execute_reply":"2024-07-06T06:37:28.799097Z"},"trusted":true},"execution_count":14,"outputs":[{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/peft/utils/other.py:102: FutureWarning: prepare_model_for_int8_training is deprecated and will be removed in a future version. Use prepare_model_for_kbit_training instead.\n  warnings.warn(\n/opt/conda/lib/python3.10/site-packages/trl/trainer/sft_trainer.py:159: UserWarning: You didn't pass a `max_seq_length` argument to the SFTTrainer, this will default to 1024\n  warnings.warn(\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/17 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"bf31d8d70af14193a1dfb4055d04133c"}},"metadata":{}},{"name":"stderr","text":"You are using 8-bit optimizers with a version of `bitsandbytes` < 0.41.1. It is recommended to update your version as a major bug has been fixed in 8-bit optimizers.\nYou're using a LlamaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='76' max='76' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [76/76 09:12, Epoch 1/2]\n    </div>\n    <table border=\"1\" class=\"dataframe\">\n  <thead>\n <tr style=\"text-align: left;\">\n      <th>Step</th>\n      <th>Training Loss</th>\n      <th>Validation Loss</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td>5</td>\n      <td>2.641800</td>\n      <td>2.447397</td>\n    </tr>\n    <tr>\n      <td>10</td>\n      <td>3.436300</td>\n      <td>2.014841</td>\n    </tr>\n    <tr>\n      <td>15</td>\n      <td>2.025200</td>\n      <td>1.749565</td>\n    </tr>\n    <tr>\n      <td>20</td>\n      <td>1.638800</td>\n      <td>1.586969</td>\n    </tr>\n    <tr>\n      <td>25</td>\n      <td>1.658600</td>\n      <td>1.468375</td>\n    </tr>\n    <tr>\n      <td>30</td>\n      <td>1.226200</td>\n      <td>1.407284</td>\n    </tr>\n    <tr>\n      <td>35</td>\n      <td>1.397100</td>\n      <td>1.314094</td>\n    </tr>\n    <tr>\n      <td>40</td>\n      <td>0.960400</td>\n      <td>1.246057</td>\n    </tr>\n    <tr>\n      <td>45</td>\n      <td>1.250300</td>\n      <td>1.215665</td>\n    </tr>\n    <tr>\n      <td>50</td>\n      <td>0.863200</td>\n      <td>1.177102</td>\n    </tr>\n    <tr>\n      <td>55</td>\n      <td>1.103000</td>\n      <td>1.144885</td>\n    </tr>\n    <tr>\n      <td>60</td>\n      <td>0.918100</td>\n      <td>1.121322</td>\n    </tr>\n    <tr>\n      <td>65</td>\n      <td>0.687900</td>\n      <td>1.094275</td>\n    </tr>\n    <tr>\n      <td>70</td>\n      <td>1.008900</td>\n      <td>1.069139</td>\n    </tr>\n    <tr>\n      <td>75</td>\n      <td>0.685100</td>\n      <td>1.044831</td>\n    </tr>\n  </tbody>\n</table><p>"},"metadata":{}},{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.\n  warnings.warn(\n/opt/conda/lib/python3.10/site-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.\n  warnings.warn(\n/opt/conda/lib/python3.10/site-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.\n  warnings.warn(\n","output_type":"stream"}]},{"cell_type":"code","source":"logging.set_verbosity(logging.CRITICAL)\nprompt = f\"[INST] <<SYS>>\\n{system_message}\\n<</SYS>>\\n\\nWhat languages are commonly spoken in Egypt? [/INST]\" \npipe = pipeline(task=\"text-generation\", model=model, tokenizer=tokenizer, max_length=200)\nresult = pipe(prompt)\nprint(result[0]['generated_text'])","metadata":{"execution":{"iopub.status.busy":"2024-07-06T06:42:54.122356Z","iopub.execute_input":"2024-07-06T06:42:54.123065Z","iopub.status.idle":"2024-07-06T06:43:25.677082Z","shell.execute_reply.started":"2024-07-06T06:42:54.123030Z","shell.execute_reply":"2024-07-06T06:43:25.676200Z"},"trusted":true},"execution_count":15,"outputs":[{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/transformers/generation/utils.py:1417: UserWarning: You have modified the pretrained model configuration to control generation. This is a deprecated strategy to control generation and will be removed soon, in a future version. Please use a generation configuration file (see https://huggingface.co/docs/transformers/main_classes/text_generation )\n  warnings.warn(\n/opt/conda/lib/python3.10/site-packages/torch/utils/checkpoint.py:61: UserWarning: None of the inputs have requires_grad=True. Gradients will be None\n  warnings.warn(\n","output_type":"stream"},{"name":"stdout","text":"[INST] <<SYS>>\nYour system message goes here\n<</SYS>>\n\nWhat languages are commonly spoken in Egypt? [/INST] Arabic is the official language, but English is widely spoken, especially in tourist areas. Other languages spoken include Egyptian Arabic, Greek, French, German, Italian, and Spanish. Knowing some basic Arabic phrases can be helpful, especially when communicating with locals. English is increasingly used in tourism and business. Understanding some basic Arabic phrases can be helpful, especially when communicating with locals.\n","output_type":"stream"}]},{"cell_type":"code","source":"from transformers import pipeline\n\nprompt = f\"[INST] <<SYS>>\\n{system_message}\\n<</SYS>>\\n\\nwhat do you know about egypt[/INST]\" # replace the command here with something relevant to your task\nnum_new_tokens = 100  # change to the number of new tokens you want to generate\n\n# Count the number of tokens in the prompt\nnum_prompt_tokens = len(tokenizer(prompt)['input_ids'])\n\n# Calculate the maximum length for the generation\nmax_length = num_prompt_tokens + num_new_tokens\n\ngen = pipeline('text-generation', model=model, tokenizer=tokenizer, max_length=max_length)\nresult = gen(prompt)\nprint(result[0]['generated_text'].replace(prompt, ''))","metadata":{"execution":{"iopub.status.busy":"2024-07-06T06:44:19.252856Z","iopub.execute_input":"2024-07-06T06:44:19.253299Z","iopub.status.idle":"2024-07-06T06:44:53.720021Z","shell.execute_reply.started":"2024-07-06T06:44:19.253254Z","shell.execute_reply":"2024-07-06T06:44:53.719169Z"},"trusted":true},"execution_count":16,"outputs":[{"name":"stdout","text":" Egypt, officially known as the Arab Republic of Egypt, is a country located in northeastern Africa. It is bordered by the Mediterranean Sea to the north, the Red Sea to the east, Sudan to the south, and Libya to the west. Egypt is home to over 100 million people, making it the most populous country in Africa and the Arab world. The capital and largest city is Cairo, which is home to over 20% of\n","output_type":"stream"}]},{"cell_type":"code","source":"# Directory to save the fine-tuned model and tokenizer\nfine_tuned_model_dir = \"SmartTour_llama-2-7b\"\n\n# Save the tokenizer\ntokenizer.save_pretrained(fine_tuned_model_dir)\n\n# Save the model configuration\nfrom transformers import AutoConfig\nconfig = AutoConfig.from_pretrained(model_name)\nconfig.save_pretrained(fine_tuned_model_dir)\n\n# Save the model itself\nfrom transformers import AutoModelForCausalLM\nmodel = AutoModelForCausalLM.from_pretrained(model_name)\nmodel.save_pretrained(fine_tuned_model_dir)\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]}]}